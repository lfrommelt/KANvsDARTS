{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c92f5c0-a93c-40fc-91a5-e6b9c1ca498b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264bad3-817e-4e19-b989-aed09e686520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from src.utils import find_k2, find_k4, PRIMITIVES, plot_pair_histograms, sample_structure, replace_xs, replace_functions, str_to_sympy, sympy_to_dataset, SamplingError, wrap_expression, sample_and_replace\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "n_functions=len(PRIMITIVES) # == rows per k\n",
    "\n",
    "# primitives combinations\n",
    "matrix1 = np.arange(6).reshape(6,1)\n",
    "\n",
    "matrix3 = np.array([\n",
    "[0, 1, 5],\n",
    "[1, 2, 0],\n",
    "[2, 3, 1],\n",
    "[3, 4, 2],\n",
    "[4, 5, 3],\n",
    "[5, 0, 4],\n",
    "])\n",
    "\n",
    "matrix2 = np.array(find_k2(matrix3))\n",
    "\n",
    "## remaining unordered pairs\n",
    "pairs = [\n",
    "    (0,3), (0,5),\n",
    "    (1,0), (1,4),\n",
    "    (2,1), (2,4),\n",
    "    (3,2), (3,5),\n",
    "    (4,0), (4,3),\n",
    "    (5,1), (5,2)\n",
    "]\n",
    "\n",
    "## takes a bit\n",
    "## matrix4 = find_k4(pairs)\n",
    "matrix4 = [[0, 3, 2, 1],\n",
    "           [1, 0, 5, 2],\n",
    "           [2, 4, 3, 5],\n",
    "           [5, 1, 4, 0]\n",
    "          ]\n",
    "\n",
    "## find last two rows by hand\n",
    "matrix4.append([3, 2, 0, 4])\n",
    "matrix4.append([4, 5, 1, 3])\n",
    "matrix4=np.array(matrix4)\n",
    "\n",
    "combinations=[matrix1, matrix2, matrix3, matrix4]\n",
    "\n",
    "## plot distribution of pairs and ordered pairs\n",
    "plot_pair_histograms([matrix2, matrix3, matrix4], n_functions)\n",
    "\n",
    "# random mapping index -> primitive\n",
    "primitives=np.array(PRIMITIVES)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(primitives)\n",
    "#print(primitives)\n",
    "\n",
    "# for now, there is no k>4 implementation\n",
    "for k in range(5):\n",
    "    ## up to three variables\n",
    "    for n_v in range(1,4):\n",
    "        # equation structure\n",
    "        if k==0:\n",
    "            equations=[\" + \".join([f\"x_{i+1}\" for i in range(n_v)]) for _ in range(n_functions)]\n",
    "        else:\n",
    "            equations=[sample_structure(k, n_v, seed=i) for i in range(n_functions)]\n",
    "            #if k==1 and (n_v==2):\n",
    "            #print(*equations, sep=\"\\n\")\n",
    "            \n",
    "            # fill in actual variables\n",
    "            equations=[replace_xs(equations[i], n_v, seed=i+1) for i in range(n_functions)]\n",
    "            \n",
    "        # needs arbitrary but distinct and replicateable seeds\n",
    "        seed_0 = k*100+n_v*10\n",
    "        linearized_equations=[wrap_expression(equation, seed) for equation, seed in zip(equations, np.arange(seed_0, seed_0+len(equations)))]\n",
    "        mult_equations=[sample_and_replace(equation, seed) for equation, seed in zip(equations, np.arange(seed_0, seed_0+len(equations)))]\n",
    "        linearized_mult_equations=[wrap_expression(sample_and_replace(equation, seed), seed) for equation, seed in zip(equations, np.arange(seed_0, seed_0+len(equations)))]\n",
    "            \n",
    "            \n",
    "        #print(*[str(eq)+\"\\n\"+str(lineq)+\"\\n\"+str(linmulteq)+\"\\n\"+str(multeq) for eq, lineq, linmulteq, multeq in zip(equations, linearized_equations, linearized_mult_equations, mult_equations)], sep=\"\\n\\n\")\n",
    "            \n",
    "            \n",
    "        for mult in (False, True):\n",
    "            \n",
    "            # no mult of the same var\n",
    "            if mult and (n_v==1):\n",
    "                continue\n",
    "            \n",
    "            for lin in (False, True):\n",
    "                #print(mult, lin)\n",
    "                if (not mult) and (not lin):\n",
    "                    equations_=equations\n",
    "                elif (not mult) and lin:\n",
    "                    equations_=linearized_equations\n",
    "                if mult and (not lin):\n",
    "                    equations_=mult_equations\n",
    "                if mult and lin:\n",
    "                    equations_=linearized_mult_equations\n",
    "\n",
    "                # fill in actual functions\n",
    "                equations_=[replace_functions(equations_[i], primitives[combinations[k-1][i]]) for i in range(n_functions)]\n",
    "\n",
    "            \n",
    "                # serialize\n",
    "                file_path = Path(f\"datasets/mult_{mult}-lin_{lin}\") / f\"equations_k{k}_nv{n_v}.txt\"\n",
    "\n",
    "                file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(file_path, \"wt\") as f:\n",
    "                    for eq in equations_:\n",
    "                        # ugly syntax, but apparently intended pythonic usage\n",
    "                        print(eq, file=f)\n",
    "                        \n",
    "                equations_debug=equations_\n",
    "                # sample datasets\n",
    "                equations_ = [str_to_sympy(eq) for eq in equations_]\n",
    "                #print(*equations, sep=\"\\n\")\n",
    "                '''for eq in equations:\n",
    "                    display(eq)'''\n",
    "                try:\n",
    "                    datasets = [sympy_to_dataset(equations_[i], n=200, domain=(1e-10, 5.0), seed=i+2)[0] for i in range(n_functions)]\n",
    "                except SamplingError:\n",
    "                    print(*equations_debug, sep=\"\\n\")\n",
    "                    print(\"\\n\")\n",
    "                    print(*equations_, sep=\"\\n\")\n",
    "                    \n",
    "                for i, dataset in enumerate(datasets):\n",
    "                    try:\n",
    "                        extrapolation_dataset_neg=sympy_to_dataset(equations_[i], n=int(200/5/2), domain=(-2.5, -1e-10), seed=i+3)[0]\n",
    "                    except SamplingError:\n",
    "                        extrapolation_dataset_pos=sympy_to_dataset(equations_[i], n=int(200/5), domain=(5.0, 7.5), seed=i+3)[0]\n",
    "                        dataset[\"extrapolation_input\"]=extrapolation_dataset_pos[\"train_input\"]\n",
    "                        dataset[\"extrapolation_label\"]=extrapolation_dataset_pos[\"train_label\"]\n",
    "                    else:\n",
    "                        extrapolation_dataset_pos=sympy_to_dataset(equations_[i], n=int(200/5/2), domain=(5.0, 7.5), seed=i+3)[0]\n",
    "                        dataset[\"extrapolation_input\"]=np.concatenate(\n",
    "                            (extrapolation_dataset_neg[\"train_input\"], extrapolation_dataset_pos[\"train_input\"]))\n",
    "                        dataset[\"extrapolation_label\"]=np.concatenate(\n",
    "                            (extrapolation_dataset_neg[\"train_label\"], extrapolation_dataset_pos[\"train_label\"]))\n",
    "\n",
    "                # datsets to pickle\n",
    "                ## no csv, because we'll need a dict for kans, anyways\n",
    "                folder = Path(f\"datasets/mult_{mult}-lin_{lin}/datasets_k{k}_nv{n_v}\")\n",
    "                folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                for i, dataset in enumerate(datasets):\n",
    "                    path=folder/f\"{i}.pkl\"\n",
    "                    with open(path, 'wb') as f:\n",
    "                        pickle.dump(dataset, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bde7f-b696-4eb4-92fd-4964e3f5579f",
   "metadata": {},
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99e64b-5f04-494e-b3b2-163d753941e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "seed=0\n",
    "\n",
    "mult=False\n",
    "lin=True\n",
    "\n",
    "for nv in (1,2,3):#(1,2,3):\n",
    "    for k in (1,2,3,4):\n",
    "        path=Path(f\"datasets/mult_{mult}-lin_{lin}/equations_k{k}_nv{nv}.txt\")\n",
    "        with open(path, 'rt') as f:\n",
    "            equation=f.read().splitlines()[seed]\n",
    "        print(equation)\n",
    "        path=Path(f\"datasets/mult_False-lin_False/datasets_k{k}_nv{nv}/{seed}.pkl\")\n",
    "        with open(path, 'rb') as f:\n",
    "            dataset=pickle.load(f)\n",
    "        print(dataset[\"extrapolation_input\"].shape)\n",
    "        print(dataset[\"extrapolation_label\"].shape)\n",
    "\n",
    "        plt.scatter([x[0] for x in dataset[\"train_input\"]], dataset[\"train_label\"])\n",
    "        plt.scatter([x[0] for x in dataset[\"extrapolation_input\"]], dataset[\"extrapolation_label\"])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b4e76-c1d8-4e5b-a40e-35047adecca2",
   "metadata": {},
   "source": [
    "# datasets df\n",
    "quick and dirty, run the experiment script (without training) and collect all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002088ed-4958-4312-b7e5-d945e7ced6e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import traceback\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils import PRIMITIVES, rmse, nrmse, round_and_simplify, sl_rmse, rmspe, r2, make_vectorized, NumericTensorJSONEncoder\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sympy as sp\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "'''import warnings\n",
    "warnings.simplefilter(\"error\")'''\n",
    "#os.environ[\"PYTHONWARNINGS\"] = \"error\"\n",
    "\n",
    "# sys.path.append(os.path.normpath(os.getcwd() + \"/autora-theorist-darts/src\"))\n",
    "\n",
    "\n",
    "#print(train)\n",
    "import src.utils\n",
    "\n",
    "success_metric=nrmse\n",
    "metric_norm=\"std\"\n",
    "success_threshold=0.2\n",
    "scale_steps=1.0#/100\n",
    "\n",
    "success_threshold = 0.01\n",
    "\n",
    "comment = \"lower th (0.01)\"\n",
    "\n",
    "'''# List the variable names we want to record\n",
    "keys = ('success_metric', 'metric_norm', 'success_threshold',\n",
    "        'scale_steps', 'comment')\n",
    "\n",
    "# Grab their current values in *this* scope.  \n",
    "# If any name is missing, Python will raise a KeyError/NameError as requested.\n",
    "vals = {k: locals()[k] for k in keys}'''\n",
    "\n",
    "import os\n",
    "\n",
    "#os.environ[\"WANDB_DEBUG\"] = \"0\"\n",
    "\n",
    "#from autora.theorist.darts.model_search import Network\n",
    "\n",
    "'''\n",
    "???\n",
    "def create_int_bins(start, end, n_edges):\n",
    "    bin_edges = np.linspace(start, end, n_edges)\n",
    "    # Round edges to integers\n",
    "    bin_edges = np.round(bin_edges).astype(int)\n",
    "    return bin_edges\n",
    "'''\n",
    "\n",
    "def stop_on_enter():\n",
    "    input(\"Press [Enter] at any time to stop the sweep...\\n\")\n",
    "    print(\"Exiting on user request.\")\n",
    "    # os._exit(1) exits the whole process, works even in wandb agent\n",
    "    os._exit(1)\n",
    "\n",
    "# random mapping index -> primitive\n",
    "primitives=np.array(PRIMITIVES)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(primitives)\n",
    "\n",
    "\n",
    "# darts hps\n",
    "config2={\n",
    "  'arch_discretization': 'softmax',\n",
    "  'arch_learning_rate_max': 0.6473364090755143,\n",
    "  'arch_momentum': 0.0017282364618274,\n",
    "  'batch_size': 20,\n",
    "  'coeff_discretization': 'max',\n",
    "  'finetune_epochs': 10,\n",
    "  'param_learning_rate_max': 0.0006232415860704,\n",
    "  'param_momentum': 1.9581913155e-06,\n",
    "  'primitives': [\"none\",\"power_two\",\"power_three\",\"exp\",\"ln\",\"reciprocal\",\"sin\"],\n",
    "  'ratio_train_val': 1.0,\n",
    "  'safety': 'ramped',\n",
    "  'steps': 720,\n",
    "  'train_output_layer': False,}\n",
    "\n",
    "config2={'Name': 'fallen-sweep-100',\n",
    " 'arch_discretization': 'softmax',\n",
    " 'arch_learning_rate_max': 0.8893474836112625,\n",
    " 'arch_momentum': 0.0066695070207013,\n",
    " 'arch_weight_decay': 0.0002014087677275,\n",
    " 'batch_size': 7,\n",
    " 'coeff_discretization': 'max',\n",
    " 'coeff_lr_min_scale': 1.0,#0.0009878495458311,\n",
    " 'finetune_epochs': 60,#8\n",
    " 'init_range': 1.0120530236610816,\n",
    " 'param_learning_rate_max': 5e-4,#3.373269557527869e-09,\n",
    " 'param_momentum': 1.509044847784784e-09,\n",
    " 'param_weight_decay': 7.2956488739e-06,\n",
    " 'pruning': 'none',\n",
    " 'ratio_train_val': 4.0,#2.0,\n",
    " 'safety': 'safe',\n",
    " 'size': 2,\n",
    " 'train_output_layer': True,\n",
    " 'primitives': [\"none\",\"power_two\",\"power_three\",\"exp\",\"ln\",\"reciprocal\",\"sin\",\"id\"],\n",
    " 'loss_fn':\"mse\"}\n",
    "\n",
    "config4={\n",
    "  'arch_discretization': 'softmax',\n",
    "  'arch_learning_rate_max': 1.6284305674366029,\n",
    "  'arch_momentum': 0.0022416242214032,\n",
    "  'batch_size': 20,\n",
    "  'coeff_discretization': 'max',\n",
    "  'finetune_epochs': 10,\n",
    "  'param_learning_rate_max': 0.0012008188033979,\n",
    "  'param_momentum': 0.0027466356242729,\n",
    "  'primitives': [\"none\",\"power_two\",\"power_three\",\"exp\",\"ln\",\"reciprocal\",\"sin\"],\n",
    "  'ratio_train_val': 1.0,\n",
    "  'safety': 'ramped',\n",
    "  'steps': 720,\n",
    "  'train_output_layer': False,\n",
    "}\n",
    "\n",
    "config6={\n",
    "  'arch_discretization': 'softmax',\n",
    "  'arch_learning_rate_max': 9.928562713532394,\n",
    "  'arch_momentum': 8.784944800579377e-09,\n",
    "  'batch_size': 20,\n",
    "  'coeff_discretization': 'max',\n",
    "  'finetune_epochs': 10,\n",
    "  'param_learning_rate_max': 0.0654276023875558,\n",
    "  'param_momentum': 5.318695973457925e-10,\n",
    "  'primitives': [\"none\",\"power_two\",\"power_three\",\"exp\",\"ln\",\"reciprocal\",\"sin\"],\n",
    "  'ratio_train_val': 1.0,\n",
    "  'safety': 'ramped',\n",
    "  'steps': 720,\n",
    "  'train_output_layer': False,}\n",
    "    \n",
    "config4=config2\n",
    "config6=config2\n",
    "\n",
    "configs=(config2, config4, config6)\n",
    "\n",
    "sizes=[1,2,3,4,5]\n",
    "        \n",
    "# conditions\n",
    "c_multiplication = (False, True)\n",
    "c_prior_knowledge = (True, False)\n",
    "c_linear_transformations = (False, True)\n",
    "\n",
    "# for simulatin results for debugging\n",
    "test_rng=np.random.default_rng(42)\n",
    "\n",
    "def main():\n",
    "    df=None\n",
    "    for multiplication in c_multiplication:\n",
    "        for prior_knowledge in c_prior_knowledge:\n",
    "            for linear_transformations in c_linear_transformations:\n",
    "                if (not linear_transformations) and (not multiplication):\n",
    "                    continue\n",
    "                # k=0 should be trivial\n",
    "                for k in range(0,5):\n",
    "                    all_failed=True\n",
    "                    for n_v in range(1,4):                            \n",
    "                        # no mult between at least two independent variables\n",
    "                        if multiplication and (n_v==1):\n",
    "                            continue\n",
    "                        for seed in range(3):\n",
    "                            # for now we concider each seed an individual trial (iid problem!)\n",
    "                            dataset_folder=Path(f\"datasets/mult_{multiplication}-lin_{linear_transformations}\")\n",
    "                            with open(dataset_folder/f\"equations_k{k}_nv{n_v}.txt\", \"rt\") as f:\n",
    "                                equations=[line.rstrip() for line in f]\n",
    "                            for combination in range(6):\n",
    "                                # for continuing when interrupted\n",
    "\n",
    "                                    \n",
    "                                # start a trial\n",
    "                                path=f\"Results/mult_{multiplication}-prior_{prior_knowledge}-lin_{linear_transformations}/k_{k}-nv_{n_v}-comb{combination}-seed{seed}.json\"\n",
    "                                #print(f\"--------------------{path}--------------------\")\n",
    "                                path=Path(path)\n",
    "                                run_logs=[]\n",
    "                                \n",
    "                                #primtitives = \n",
    "                                \n",
    "                                text        = equations[combination]\n",
    "                                substrings  = ['ln', 'exp', '1/', \"**2\", \"**3\", \"sin\"]\n",
    "\n",
    "                                pattern = '|'.join(map(re.escape, substrings))     #  foo|bar|baz  (escaped)\n",
    "\n",
    "                                actual_prims = [m.group(0) for m in re.finditer(pattern, text)]\n",
    "                                \n",
    "                                print(equations[combination])\n",
    "\n",
    "                                # for k=0, all structures are the same\n",
    "                                if k==0 and (combination>0) and (not linear_transformations):\n",
    "                                    continue\n",
    "                                for size in (1,2):#range(max(k-1,1),k+3):\n",
    "                                    data_dict={\"mult\":multiplication,\"linear\":linear_transformations, \n",
    "                                               \"prior knowledge\": prior_knowledge,\"k\":k, \"n_v\": n_v, \"structure\": actual_prims, \"seed\": seed, \"size factor\": size\n",
    "                                              }\n",
    "                                    if df is None:                 # first iteration → create the DataFrame\n",
    "                                        df = pd.DataFrame([data_dict])\n",
    "                                    else:                          # subsequent iterations → append one row\n",
    "                                        df.loc[len(df)] = data_dict \n",
    "                    \n",
    "    return df\n",
    "\n",
    "\n",
    "df=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f6d69-5c24-4073-a68d-684efc04e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
